---
title: "NLP Project: Sarcasm detection in news headlines"
date: 2020-05-13
tags: [NLP, Machine Learning, Sentiment analysis]
header:
  image: "/images/sarcasm3.jpg"
  excerpt: "NLP"
---

Few weeks ago i've  finished a 75 hours data science course and chose this dataset as my graduation project. 
I was fascinated about the idea that with a little bit of analysis, data preparation and few machine learning algorithms, computers can read text information and decide which part of it is sarcastic and which is serious. 
Sometimes even for people in their daily life it's a really challenging task.

*This is my first full project so you can expect some slightly bad and ineffective decisions. Everyone should start somewhere and i hope that with every new project I'll get better and better. And now without further ado let's go straight to the point.*

I used this high quality dataset from kaggle:
[News Headlines Dataset For Sarcasm Detection](https://www.kaggle.com/rmisra/news-headlines-dataset-for-sarcasm-detection)

So the basic steps of this project is:
* Data preparation and cleaning
* Vocabulary Creation and vectorization
* Apply few different machine learning models
* Result analysis

## Data Preparation

First let's import all the necessary libraries and prepare training and testing data.

```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sn
import re

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
from sklearn import metrics
```

```python
df_con = pd.read_json('Sarcasm_Headlines_Dataset.json', lines=True)
df_test = pd.read_json('Sarcasm_Headlines_Dataset_v2.json', lines=True)
```

This is how our training data looks like:
<img src="/images/df_con.PNG" alt="">

After data analysis i've noticed that there is a lot of swear words in sarcastic headlines so one of my tasks was to censor them, 
the second most important thing was to extract the website title from article links, because usually only this part can decide if the headline is sarcastic or not. 
So based on those observations I've created a function that will change every swear word into default value as 'swear_word', extract the website name and add it to the end of the headline, then delete ‘article_link’ column.

```python
bad_words_lst = ['fucking', 'fuck', 'shit', 'asshole', 'shitty', 'fucks']

def data_prep(df):
    url = df['article_link']
    url = url.str.replace(".","")
    df['headline'] = df['headline'] + ' ' + url.str.split('https://').str[1].str.split('www').str[-1].str.split('net').str[0].str.split('com').str[0]
    df = df.drop(['article_link'], axis=1)

    big_regex = re.compile('|'.join(map(re.escape, bad_words_lst)))
    df['headline'] = df['headline'].apply(lambda x: big_regex.sub("swear_word", x) if any(i in x for i in bad_words_lst) else x)
    df['headline'] = df['headline'].str.replace('[0-9]+','')  
    
    return df
```

Apply function to both training and testing datasets

```python
df_con = data_prep(df_con)
df_test = data_prep(df_test)
```

This is how our training dataset looks like after cleaning process

```python
df_con
```

<img src="/images/df_con2.PNG" alt="">


And this is example of censored headline with website name in the end

```python
df_con['headline'][72]
```
`Output: sick swear_words line up to gape at dead body theonion`

Now I should investigate sarcastic and not sarcastic headline distribution. In the best scenario 50% of headlines would be sarcastic and another 50% not sarcastic. So that way I'll be sure that my machine learning model is not biased on dominating label.

```python
sn.set(style="whitegrid")
sn.countplot(df_con.is_sarcastic)
plt.title('Sarcasm vs Non-sarcasm')
```

<img src="/images/distribution.PNG" alt="">

```python
df_con.is_sarcastic.value_counts()
```
`Output:
0   14985
1    11724
Name: is_sarcastic, dtype: int64`

Lucky me! Sarcastic and not sarcastic data is distributed pretty even. There are 11724 sarcastic and 14985 not sarcastic entries. So without a model my guess accuracy can be 56%.

Before creating vocabulary and vectorizing I split my data into train/test groups. Because i want to create vocabulary only from training data.

```python
X_train = df_con['headline']
y_train = df_con['is_sarcastic']

X_test = df_test['headline']
y_test = df_test['is_sarcastic']
```

## Vocabulary Creation and vectorization

In this project I decided to use TfidfVectorizer, because it’s a big database with a lot of repeating words so every word can get more accurate values than in CountVectorizer. Also i setup some parameters:
Removed english stop words
For better prediction vocabulary included one word and two words groups
Removed words that were repeated in more than 80% headlines
Removed words that were repeated in less than 10 headlines

```python
vect = TfidfVectorizer(stop_words='english', ngram_range=(1,2), max_df=0.8, min_df=10)
X_train_sm = vect.fit_transform(X_train)
X_test_sm = vect.transform(X_test)
```

```python
X_train_sm
```

`Output: <26709x4216 sparse matrix of type '<class 'numpy.float64'>'
	with 167576 stored elements in Compressed Sparse Row format>`

So after vectorization my vocabulary consisted of 4216 words or words groups

```python
pd.DataFrame(X_train_sm.toarray(), columns=vect.get_feature_names())
```

<img src="/images/vectorized.PNG" alt="">

Here I previewed words with the biggest weight and impact when deciding if the headline is sarcastic. 
For example if in the machine learning model creation process huffingtonpost word will be treated as not sarcastic in most cases it can have a big impact on the whole headline being labeled as not sarcastic.

```python
word_list = vect.get_feature_names();    
count_list = X_train_sm.toarray().sum(axis=0) 

vocab =  dict(zip(word_list,count_list))
vocab = pd.DataFrame.from_dict(vocab, orient='index',  columns=['Count'])
vocab = vocab.sort_values(by='Count', ascending=False)
vocab.head(15)
```

<img src="/images/bigwords.PNG" alt="">

## Machine Learning Models

In this project i decided to use three popular machine learning models:

* Logistic Regression
* Random Forest
* Naive Bayes

The idea of this part is to compare different model accuracy with website title information and without it, because this feature drastically changes accuracy results, then evaluate features with biggest importance and compare them. 

### Logistic Regression


```python
from sklearn.linear_model import LogisticRegression
model_lr = LogisticRegression(C=10)
model_lr.fit(X_train_sm, y_train)
```

```python
y_pred_class_lr = model_lr.predict(X_test_sm)
metrics.accuracy_score(y_test, y_pred_class_lr)
```
`accuracy with website title information 0.9991963380970684
 accuracy without website title information 0.8500995841923198`

 We can see that mentioning website title increases model accuracy by 14.9% and if you have a dataset where most of the headlines are from Huffington Post or Onion your model predictions will be pretty accurate, 
however if your database consists of a lot of different and smaller news portals predictions may not be so accurate.

The goal of next part is to find words with the biggest impact on deciding if the headline is sarcastic or not. 
For logistic regression and random forest models I leave this simple and do not define how sarcastic it is, this only shows it’s impact on model prediction. 
In Naive Bayes I did a deeper analysis and calculated sarcasm ratio for each word.

 Feature importance analysis:

 ```python
X_train_tokens_lr = vect.get_feature_names()
abs_weights = np.abs(model_lr.coef_)[0, :]
tokens_lr = pd.DataFrame({'token':X_train_tokens_lr, 'Feature_Importance':abs_weights}).set_index('token')
tokens_lr = tokens_lr.sort_values(by=['Feature_Importance'], ascending = False)
```

 ```python
 tokens_lr
```

<img src="/images/feature_importance_web.PNG" alt="">

 ```python
sn.barplot(x= tokens_lr.index[:20], y=tokens_lr.Feature_Importance[:20], data=tokens_lr)
plt.xticks(rotation=90)
```
<img src="/images/feature_hist.PNG" alt="">

After feature importance analysis in logistic regression I noticed that most important words are website titles and their variations. 
And the gap between them and the rest of the words is huge. So in many cases only the website title can decide if the headline is sarcastic or not. 

As an alternative I made the same analysis for data without website data so we can more clearly analyse different words and their importance. 
That’s how the same data looks without website titles:

<img src="/images/feature_importance_lr.PNG" alt=""> 
<img src="/images/feature_hist_lr.PNG" alt="">

As we can see *swear_word, nation, area, clearly* introduces have the biggest impact for our predictions and *heart, eating, marriage, west, military* have lowest impact. 
In this case the gap is not so drastic and we can notice a slight decrease of feature importance values.

### Random Forest

I made the same steps with a random forest classifier. Only this time I used gridsearchcv for getting best parameters such as max_depth and min_samples_split.

 ```python
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import GridSearchCV
model_rf = RandomForestClassifier()
i_range = list(range(2, 21))
param_grid_rf = dict(max_depth = i_range, min_samples_split = i_range)
grid_rf = GridSearchCV(model_rf, param_grid_rf, cv=10, scoring='accuracy', return_train_score=False, n_jobs = -1)
grid_rf.fit(X_train_sm, y_train)
```

 ```python
print(grid_rf.best_params_)
```

`{'max_depth': 13, 'min_samples_split': 13}`

 ```python
model_rf = RandomForestClassifier(max_depth = 13, min_samples_split = 13)
model_rf.fit(X_train_sm, y_train)
y_pred_class_rf = model_rf.predict(X_test_sm)
metrics.accuracy_score(y_test, y_pred_class_rf)
```

`accuracy with website title information 0.9944093085013452
 accuracy without website title information 0.6617981061532549`

 Accuracy difference at this point way more drastic from logistic regression. We get pretty good results while using website titles, but without this feature our accuracy score drops down by 33%. 
 So at this point I decided not to use a random forest classifier in my project.

Let's check feature importance and see how it differs from logistic regression.

  ```python
X_train_tokens_rf = vect.get_feature_names()
model_rf.feature_importances_
tokens_rf = pd.DataFrame({'token':X_train_tokens_rf, 'Feature_Importance':model_rf.feature_importances_}).set_index('token')
tokens_rf = tokens_rf.sort_values(by=['Feature_Importance'], ascending = False)

tokens_rf.head(20)
```

<img src="/images/feature_importance_web_rf.PNG" alt="">

  ```python
sn.barplot(x= tokens_rf.index[:20], y=tokens_rf.Feature_Importance[:20], data=tokens_rf)
plt.xticks(rotation=90)
```

<img src="/images/feature_hist_rf.PNG" alt="">

We can see that at this point the gap between most important features and others is even more bigger than in logistic regression. Let’s try to remove the website title.

<img src="/images/feature_importance_rf.PNG" alt="">
<img src="/images/feature_hist_rf2.PNG" alt="">

After removing website titles I was able to decrease a big gap between different words. 
Anyway feature importance in this model  is different, most important words are *man, area, trump, nation, donald trump* and least important words are *study, local, finds, old, woman*.

### Naive Bayes

The last model that i used was Naive Bayes and i think this is the best choice for my sentiment analysis project. This really helps me to calculate sarcasm ratio of every single word. 

  ```python
from sklearn.naive_bayes import MultinomialNB
model = MultinomialNB(alpha=0.01)
model.fit(X_train_sm, y_train)
y_pred_class = model.predict(X_test_sm)
metrics.accuracy_score(y_test, y_pred_class)
```

`accuracy with website title information 0.9867570495125616
 accuracy without website title information 0.8165204933785247`

 Prediction accuracy while using website title was 1% lower than in previous models and difference between this prediction and one without website title is 17%, 
 it’s a 3.1% higher than logistic regression so in this case for prediction i’ll use logistic regression model, but for deeper analysis i prefer Naive Bayes model.


## Deeper Naive Bayes result analysis

### Confusion matrix

So at first let’s generate confusion matrix to see prediction accuracy for different labels. 

  ```python
metrics.confusion_matrix(y_test, y_pred_class)
```
`array([[14844,   141],
       [  238, 13396]], dtype=int64)`

As we can see we have 141 false positive headlines it’s not sarcastic headlines falsely indicated as sarcastic. 
 It’s pretty good result because false positive headlines is only 0.94% of all not sarcastic headlines.


  ```python
X_test[y_test < y_pred_class]
```

```101      study finds american diets are poor (but impro...
102      after decades of effort, chemists overseas rep...
205      to the obese woman crying at the picnic table ...
308      death of mentally ill woman in police custody ...
377      fda approves nasal-spray version of overdose d...
Name: headline, Length: 141, dtype: object```

We can explore examples for better understanding why those lines were treated as sarcastic.

  ```python
X_test[101]
```

`output: study finds american diets are poor (but improving!) huffingtonpost`

Also we have 238 false negative headlines it’s sarcastic headlines falsely treated as not sarcastic. 
That’s pretty good news because false negative headlines are only 1.745% of all sarcastic headlines.

  ```python
X_test[y_test > y_pred_class]
```

```347      global-warming crisis makes for delightful mid...
486      jimmy fallon six tantalizing months from disap...
581      inconsolable sarah palin opens up about sacha ...
638      new instant lottery game features three ways t...
716      putin condemns ukrainian people's unprovoked ,...                     
Name: headline, Length: 238, dtype: object```

Example of false negative headline.

  ```python
X_test[347]
```

`output: global-warming crisis makes for delightful mid-february afternoon theonion`

### Sarcasm ratio in different words

Now we calculate the sarcasm ratio of every word from our vocabulary so we can better understand what part of the headline was most important in deciding whether it’s sarcastic or not.

This is how we can check all the words in our vocabulary

  ```python
X_train_tokens = vect.get_feature_names()
print(X_train_tokens[0:50])
```

`Output: ['abandoned', 'abc', 'ability', 'able', 'abortion', 'abroad', 'absence', 'absolutely', 'abuse', 'abused', 'abusive', 'academy', 'accept', 'acceptance', 'accepting', 'accepts', 'access', 'accident', 'accidentally', 'according', 'account', 'accusations', 'accused', 'accuses', 'acquires', 'act', 'acting', 'acting like', 'action', 'activist', 'activists', 'actor', 'actors', 'actress', 'actual', 'actually', 'ad', 'ad huffingtonpost', 'adam', 'add', 'added', 'addiction', 'address', 'adds', 'adele', 'administration', 'admit', 'admits', 'adorable', 'ads']`


  ```python
model.feature_count_
```
While using feature_count_ we can analyse label weight in every of our words. For example, first word is *abandoned* it’s not sarcastic rate is first element of first list in this case *4.83092847* and 
sarcastic rate is the first element of second list *2.32852624*. So we can clearly see what impact every different word will have in the prediction process.

`Output: array([[4.83092847, 3.9066042 , 1.924182  , ..., 3.88694424, 2.45450549,
        3.75873655],
       [2.32852624, 1.92555452, 4.26382144, ..., 4.57648396, 9.06628516,
        4.35841063]])`

Now let’s create dataframe and normalize numbers for better readability

  ```python
non_sarcastic_token_count = model.feature_count_[0, :]
sarcastric_token_count = model.feature_count_[1, :]
tokens = pd.DataFrame({'token':X_train_tokens, 'Not-Sarcastic':non_sarcastic_token_count, 'Sarcastic':sarcastric_token_count}).set_index('token')
tokens.head(10)
```

<img src="/images/feature_importance_web_nb.PNG" alt="">

  ```python
tokens['Not-Sarcastic'] = tokens['Not-Sarcastic'] + 1
tokens['Sarcastic'] = tokens['Sarcastic'] + 1
tokens['Not-Sarcastic'] = tokens['Not-Sarcastic'] / model.class_count_[0]
tokens['Sarcastic'] = tokens['Sarcastic'] / model.class_count_[1]
tokens.sample(10)
```

<img src="/images/feature_importance_web_nb2.PNG" alt="">

  ```python
tokens['sarcasm_ratio'] = tokens['Sarcastic'] / tokens['Not-Sarcastic']
tokens.sample(10)
sarcasm_ratio_df = tokens.sort_values('sarcasm_ratio', ascending=False)
not_sarcasm_ratio_df = (tokens[['sarcasm_ratio']]) * -1
sarcasm_ratio_df
```

<img src="/images/feature_importance_web_nb3.PNG" alt="">

  ```python
sarcasm_ratio_dict = dict(zip(sarcasm_ratio_df.index.tolist(), sarcasm_ratio_df['sarcasm_ratio'].tolist()))
not_sarcasm_ratio_dict = dict(zip(not_sarcasm_ratio_df.index.tolist(), not_sarcasm_ratio_df['sarcasm_ratio'].tolist()))

from wordcloud import WordCloud
wc = WordCloud(width=1200, height=800, max_words=50).generate_from_frequencies(sarcasm_ratio_dict)
plt.imshow(wc, interpolation='bilinear')
plt.axis("off")
plt.show()
```

<img src="/images/sarcastic_wc_web.png" alt="">

  ```python
wc_not = WordCloud(width=800, height=400, max_words=50).generate_from_frequencies(not_sarcasm_ratio_dict)
plt.imshow(wc_not, interpolation='bilinear')
plt.axis("off")
plt.show()
```

<img src="/images/notsarcastic_wc_web.png" alt="">

feature importance results without website title included:

<img src="/images/feature_importance_nb3.PNG" alt="">

wordcloud of sarcastic words:
<img src="/images/sarcastic_wc.png" alt="">

wordcloud of not sarcastic words:
<img src="/images/notsarcastic_wc.png" alt="">

### Testing sarcasm detection in custom headline

  ```python
headline_test = ["Construction Worker Performs Incredibly Intricate Tricks With His Giant Excavating Machine"]
headline_test_t = vect.transform(headline_test)
```

  ```python
print (headline_test_t)
```

`Output:  
  (0, 4165)	0.4757453163164351
  (0, 2215)	0.4898925335550495
  (0, 1491)	0.4842111563934077
  (0, 769)	0.5470020617823563`

  ```python
if model_lr.predict(headline_test_t) == 1:
    print('Headline is sarcastic')
else:
    print('Headline is not sarcastic')
```
`Output: 'Headline is not sarcastic'`
---
title: "NLP Project: Sarcasm detection in news headlines"
date: 2020-05-13
tags: [NLP]
header:
  image: "/images/sarcasm3.jpg"
  excerpt: "NLP"
---

Few weeks ago i've  finished a 75 hours data science course and chose this dataset as my graduation project. 
I was fascinated about the idea that with a little bit of analysis, data preparation and few machine learning algorithms, computers can read text information and decide which part of it is sarcastic and which is serious. 
Sometimes even for people in their daily life it's a really challenging task.

*This is my first full project so you can expect some slightly bad and ineffective decisions. Everyone should start somewhere and i hope that with every new project I'll get better and better. And now without further ado let's go straight to the point.*

I used this high quality dataset from kaggle:
[News Headlines Dataset For Sarcasm Detection](https://www.kaggle.com/rmisra/news-headlines-dataset-for-sarcasm-detection)

So the basic steps of this project is:
* Data preparation and cleaning
* Vocabulary Creation and vectorization
* Apply few different machine learning models
* Result analysis

## Data Preparation

First let's import all the necessary libraries and prepare training and testing data.

```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sn
import re

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
from sklearn import metrics
```

```python
df_con = pd.read_json('Sarcasm_Headlines_Dataset.json', lines=True)
df_test = pd.read_json('Sarcasm_Headlines_Dataset_v2.json', lines=True)
```

This is how our training data looks like:
<img src="/images/df_con.PNG" alt="">

After data analysis i've noticed that there is a lot of swear words in sarcastic headlines so one of my tasks was to censor them, 
the second most important thing was to extract the website title from article links, because usually only this part can decide if the headline is sarcastic or not. 
So based on those observations I've created a function that will change every swear word into default value as 'swear_word', extract the website name and add it to the end of the headline, then delete ‘article_link’ column.

```python
bad_words_lst = ['fucking', 'fuck', 'shit', 'asshole', 'shitty', 'fucks']

def data_prep(df):
    url = df['article_link']
    url = url.str.replace(".","")
    df['headline'] = df['headline'] + ' ' + url.str.split('https://').str[1].str.split('www').str[-1].str.split('net').str[0].str.split('com').str[0]
    df = df.drop(['article_link'], axis=1)

    big_regex = re.compile('|'.join(map(re.escape, bad_words_lst)))
    df['headline'] = df['headline'].apply(lambda x: big_regex.sub("swear_word", x) if any(i in x for i in bad_words_lst) else x)
    df['headline'] = df['headline'].str.replace('[0-9]+','')  
    
    return df
```

Apply function to both training and testing datasets

```python
df_con = data_prep(df_con)
df_test = data_prep(df_test)
```

